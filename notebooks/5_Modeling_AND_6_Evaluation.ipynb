{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. CRISP-DM: Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T00:03:50.839809Z",
     "iopub.status.busy": "2024-07-14T00:03:50.837809Z",
     "iopub.status.idle": "2024-07-14T00:04:44.709717Z",
     "shell.execute_reply": "2024-07-14T00:04:44.709717Z"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# ## 5. CRISP-DM: Modeling\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, accuracy_score, recall_score, precision_score, f1_score\n",
    "import joblib\n",
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "import seaborn as sns\n",
    "\n",
    "# Settings for plotting\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "get_ipython().run_line_magic('config', \"InlineBackend.figure_format = 'retina'  # High resolution figures\")\n",
    "\n",
    "def prepare_data_for_modeling(train, test):\n",
    "    # Data preparation\n",
    "    X = train.drop([\"WnvPresent\"], axis=1)\n",
    "    y = train['WnvPresent']\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, test\n",
    "\n",
    "# Load datasets\n",
    "train_cleaned = pd.read_csv('../data/cleaned_train.csv')\n",
    "test_cleaned = pd.read_csv('../data/cleaned_test.csv')\n",
    "\n",
    "def prepare_models():\n",
    "    models = {\n",
    "        'Majority Classifier': DummyClassifier(strategy=\"most_frequent\"),\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "        'Decision Tree': DecisionTreeClassifier(),\n",
    "        'Random Forest': RandomForestClassifier()\n",
    "    }\n",
    "    return models\n",
    "\n",
    "def train_models(models, X_train, y_train):\n",
    "    trained_models = {}\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        trained_models[name] = model\n",
    "    return trained_models\n",
    "\n",
    "def plot_confusion_matrix(ax, y_true, y_pred, class_names, classifier_name):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "    cm_perc = cm / cm_sum.astype(float) * 100\n",
    "    annot = np.empty_like(cm).astype(str)\n",
    "    \n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            c = cm[i, j]\n",
    "            p = cm_perc[i, j]\n",
    "            s = cm_sum[i]\n",
    "            if i == j:\n",
    "                annot[i, j] = f'{c}\\n{p:.1f}%\\n{\"TN\" if i == 0 else \"TP\"}'\n",
    "            elif c == 0:\n",
    "                annot[i, j] = f'0\\n{\"FP\" if i == 0 else \"FN\"}'\n",
    "            else:\n",
    "                annot[i, j] = f'{c}\\n{p:.1f}%\\n{\"FP\" if i == 0 else \"FN\"}'\n",
    "\n",
    "    cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "    sns.heatmap(cm_df, annot=annot, fmt='', cmap='viridis', cbar=False, ax=ax)\n",
    "    ax.set_ylabel('Actual')\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_title(f'{classifier_name}\\nTN: {tn}, FP: {fp}, FN: {fn}, TP: {tp}')\n",
    "    ax.set_xticklabels(class_names, rotation=45)\n",
    "    ax.set_yticklabels(class_names, rotation=0)\n",
    "\n",
    "def plot_roc_curve(ax, y_true, y_proba, classifier_name):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
    "    auc_score = roc_auc_score(y_true, y_proba)\n",
    "    ax.plot(fpr, tpr, label=f'{classifier_name} (AUC = {auc_score:.2f})')\n",
    "    ax.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title('ROC Curve')\n",
    "    ax.legend(loc='lower right')\n",
    "\n",
    "def evaluate_models(trained_models, X_test, y_test, plot_cm=True, plot_roc=True):\n",
    "    if not trained_models:\n",
    "        print(\"No trained models to evaluate.\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame if no models are provided\n",
    "    \n",
    "    results = {}\n",
    "    class_names = ['NO WNV', 'WNV']\n",
    "    fig_cm, axes_cm = plt.subplots(1, len(trained_models), figsize=(5 * len(trained_models), 5))\n",
    "    fig_roc, ax_roc = plt.subplots(figsize=(10, 7))\n",
    "    \n",
    "    for i, (name, model) in enumerate(trained_models.items()):\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "        results[name] = {\n",
    "            'Accuracy': accuracy_score(y_test, y_pred),\n",
    "            'ROC AUC': roc_auc_score(y_test, y_prob) if y_prob is not None else 'N/A',\n",
    "            'F1 Score': f1_score(y_test, y_pred),\n",
    "            'Precision': precision_score(y_test, y_pred),\n",
    "            'Recall': recall_score(y_test, y_pred)\n",
    "        }\n",
    "        \n",
    "        if plot_cm:\n",
    "            plot_confusion_matrix(axes_cm[i], y_test, y_pred, class_names, name)\n",
    "        \n",
    "        if plot_roc and y_prob is not None:\n",
    "            plot_roc_curve(ax_roc, y_test, y_prob, name)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return pd.DataFrame(results).T\n",
    "\n",
    "def hyperparameter_tuning(models, X_train, y_train):\n",
    "    \n",
    "    param_grid = {\n",
    "        'Logistic Regression': {\n",
    "            'C': [0.1, 1, 10],  \n",
    "            'solver': ['liblinear'],  \n",
    "            'penalty': ['l2']  \n",
    "        },\n",
    "        'Decision Tree': {\n",
    "            'max_depth': [None, 10, 20],  \n",
    "            'min_samples_split': [2, 10],  \n",
    "            'min_samples_leaf': [1, 5]  \n",
    "        },\n",
    "        'Random Forest': {\n",
    "            'n_estimators': [100, 200],  \n",
    "            'max_depth': [None, 10, 20],  \n",
    "            'min_samples_split': [2, 10],  \n",
    "            'min_samples_leaf': [1, 5]  \n",
    "        }\n",
    "    }\n",
    "    \n",
    "    best_models = {}\n",
    "    for name, model in models.items():\n",
    "        if name in param_grid:\n",
    "            grid_search = GridSearchCV(model, param_grid=param_grid[name], cv=3, scoring='roc_auc')\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            best_models[name] = grid_search.best_estimator_\n",
    "            print(f\"Best parameters for {name}: {grid_search.best_params_}\")\n",
    "    \n",
    "    return best_models\n",
    "\n",
    "def cross_validate_model(model, X_train, y_train):\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "    print(f'Cross-Validation Scores for {model}: {scores}')\n",
    "    print(f'Mean CV Score: {np.mean(scores):.2f}')\n",
    "\n",
    "def main_workflow():\n",
    "    display(Markdown(f\"## Training and Evaluation without SMOTE\"))\n",
    "\n",
    "    # Load datasets\n",
    "    train_cleaned = pd.read_csv('../data/cleaned_train.csv')\n",
    "    test_cleaned = pd.read_csv('../data/cleaned_test.csv')\n",
    "    X_train, X_test, y_train, y_test, test_encoded = prepare_data_for_modeling(train_cleaned, test_cleaned)\n",
    "\n",
    "    # Prepare and train models\n",
    "    models = prepare_models()\n",
    "    trained_models = train_models(models, X_train, y_train)\n",
    "    \n",
    "    # Evaluate initial models and display results\n",
    "    print(\"Evaluating initial models:\")\n",
    "    results_df = evaluate_models(trained_models, X_test, y_test, plot_cm=True, plot_roc=True)\n",
    "    display(results_df)  # This will display the evaluation metrics in tabular form\n",
    "\n",
    "    # Perform hyperparameter tuning and re-train models\n",
    "    best_models = hyperparameter_tuning(models, X_train, y_train)\n",
    "    best_trained_models = train_models(best_models, X_train, y_train)\n",
    "    \n",
    "    # Evaluate hyperparameter-tuned models and display results\n",
    "    print(\"Evaluating hyperparameter-tuned models:\")\n",
    "    best_results_df = evaluate_models(best_trained_models, X_test, y_test, plot_cm=True, plot_roc=True)\n",
    "    display(best_results_df)  # This will display the evaluation metrics in tabular form\n",
    "\n",
    "    # Select and cross-validate the best model (adjust as necessary to pick the actual best model)\n",
    "    if 'Random Forest (estimators=200, depth=20)' in best_trained_models:\n",
    "        best_model = best_trained_models['Random Forest (estimators=200, depth=20)']\n",
    "    else:\n",
    "        best_model = RandomForestClassifier(n_estimators=200, max_depth=20)\n",
    "        best_model.fit(X_train, y_train)  # Ensure the model is fitted\n",
    "\n",
    "    cross_validate_model(best_model, X_train, y_train)\n",
    "\n",
    "    # Create submission CSV using the best model\n",
    "    test_predictions = best_model.predict_proba(test_encoded)[:, 1]\n",
    "    test_predictions = np.round(test_predictions, 2)\n",
    "    submission = pd.DataFrame({\n",
    "        'Id': test_cleaned['Id'],\n",
    "        'WnvPresent': test_predictions\n",
    "    })\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    print(\"Submission file created successfully.\")\n",
    "\n",
    "# Run the workflow\n",
    "main_workflow()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix Terminology\n",
    "\n",
    "- **True Negatives (TN):** The number of instances correctly predicted as not having the condition.\n",
    "- **True Positives (TP):** The number of instances correctly predicted as having the condition\n",
    "\n",
    "- **False Positives (FP):** The number of instances incorrectly predicted as having the condition when they do not.\n",
    "- **False Negatives (FN):** The number of instances incorrectly predicted as not having the condition when they do.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
